{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Super Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Mahima Chowdary Maddineni, Dec 12, 2022* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem statement is well-known. There were issues with distorted images, so efforts would have been made to improve image quality. We can use a variety of methods to enhance image quality. This project employs two methodologies: ESGAN and Autoencoders.\n",
    "\n",
    "Super-resolution refers to improving the resolution of an image. One of the methods is by using autoencoders. An autoencoder is just an unsupervised neural network that, by design, learns how to compress and reconstruct data with the least amount of data loss. However, optimization-based super-resolution often lacks high-frequency details and fails to match the accuracy expected at the higher resolution. GANs use a perceptual loss function which consists of an adversarial loss and a content loss which helps in producing better output. \n",
    "\n",
    "**Dataset**:\n",
    "\n",
    "[Large-scale CelebFaces Attributes (CelebA) Dataset.](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)\n",
    "The CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset comprised of 202,599 celebrity images with 40 labels. This collection of images depicts a variety of poses and crowded backgrounds. There is a great deal of diversity, information, and annotations on CelebA.\n",
    "\n",
    "[Berkeley Segmentation Dataset.](https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/)\n",
    "The dataset BSD200 consists of 200 images. The dataset is composed of a large variety of images ranging from natural images to object-specific such as plants, people, food etc. \n",
    "\n",
    "\n",
    "Because of the large size of the CelebA dataset, the model has access to a wide variety of examples for training. However, focusing all of the model's training on faces might not be the best idea, and at least one of the models could gain something from this. As a consequence of this, I decided to work with a relatively smaller BSDS200 dataset that contains a number of different images.\n",
    "work with.\n",
    "\n",
    "Low resolution images are produced by preprocessing and reducing the resolution of these images. During training, the loss of these low resolution images is compared to the loss of high resolution images, and the model is trained to produce hr images.\n",
    "\n",
    "By combining a deep network with an adversary network, GAN produces images with higher resolution. During training, a high-resolution (HR) image is downsampled to a low-resolution (LR) image (LR). A GAN generator transforms low-resolution images into high-resolution ones (SR). We use a discriminator and backpropagate the GAN loss to divide the HR images for training the discriminator and generator.\n",
    "\n",
    "Autoencoders add some white noise to the image prior to training. While training, they compare the error to the original image. The low resolution image and the predicted image are compared at the conclusion to determine the psnr and mse error between the two images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ESRGAN**\n",
    "\n",
    "Enhanced Super-Resolution Generative Adversarial Network (ESRGAN) is the improved version of the Super-Resolution Generative Adversarial Network (SRGAN). The GAN's top-level architecture consists of two networks: the generator network and the discriminator network. The generator network attempts to generate fake data, while the discriminator network attempts to differentiate between real and fake data, thereby assisting the generator in producing more realistic data. The ESRGAN has the same basic architecture as the SRGAN with some modifications. Residual in Residual Dense Block (RRDB) is a feature of ESRGAN that combines multi-level residual networks and dense connections without Batch Normalization. Because the ESRGAN makes use of a relativistic discriminator, it is able to more accurately approximate the probability that an image is either real or fake, which ultimately leads to better results. During adversarial training, the generator makes use of a linear combination of Pixel-wise absolute difference between real and fake images, Perceptual difference between real and fake images using a pre-trained VGG19 network, and Relativistic average loss between real and fake images function. All of these are used to compare real and fake images.\n",
    "\n",
    "The majority of methods for super-resolution involve training a network with supervised high-resolution and low-resolution images respectively. It is not possible to find pairs from the real world. The majority of jobs make use of bicubic down-sampling in order to achieve a low resolution.\n",
    "\n",
    "<img src=\"results/original_e_bsds.jpg\" alt=\"This is the original image from bsds200 dataset.\" /> \n",
    "<img src=\"results/lr_e_bsds.jpg\" alt=\"This is the low resolution image from bsds200 dataset.\" /> \n",
    "\n",
    "The images above were extracted from the BSDS200 dataset. The first image is the original image, and the second image is a low-resolution downsampled version of the original.\n",
    "\n",
    "<img src=\"results/original_e_celeb.jpg\" alt=\"This is the original image from bsds200 dataset.\" /> \n",
    "<img src=\"results/lr_e_celeb.jpg\" alt=\"This is the low resolution image from bsds200 dataset.\" /> \n",
    "The images above were taken from the celebA dataset. The first image is the original image, and the second image is a low-resolution downsampled version of the original.\n",
    "\n",
    "These low resolution images were used as input for the model, and the original images are output. ESRGAN upscales the Low Resolution (LR) image to High Resolution (HR) by a factor of 4. Adam optimizer is used with default values for optimization.\n",
    "\n",
    " \n",
    "**Autoencoders**\n",
    "\n",
    "Auto-encoders are generative models employed for unsupervised learning. By adding random noise to its inputs and forcing the autoencoder to recover the original, noise-free data, the autoencoder is forced to discover useful features. Because the input contains random noise, the autoencoder cannot simply copy the input to its output in this manner. We are requesting that it remove noise and generate the underlying meaningful data. This device is known as a denoising autoencoder.\n",
    "\n",
    "As this is a task involving the enhancement of image resolution, we will distort our images and use them as input images. Our output images will include the original images. The plan is to feed these distorted images to our model and train it to reconstruct the original image. Due to the size of the celebA dataset, only 1000 images were used to train the model.\n",
    "\n",
    "\n",
    "<img src=\"results/original_a_bsds.jpg\" alt=\"This is the original image from bsds200 dataset.\" /> \n",
    "<img src=\"results/lr_a_bsds.jpg\" alt=\"This is the low resolution image from bsds200 dataset.\" /> \n",
    "The images above were extracted from the BSDS200 dataset. The first image is the original image, and the second image is a low-resolution achieved by adding noise to the original.\n",
    "\n",
    "\n",
    "<img src=\"results/original_a_celeb.jpg\" alt=\"This is the original image from bsds200 dataset.\" /> \n",
    "<img src=\"results/lr_a_celeb.jpg\" alt=\"This is the low resolution image from bsds200 dataset.\" /> \n",
    "The images above were extracted from the celeb dataset. The first image is the original image, and the second image is a low-resolution achieved by adding noise to the original.\n",
    "\n",
    "\n",
    "**Evaluation Metrics**\n",
    "\n",
    "The mean-square error (MSE) and the peak signal-to-noise ratio (PSNR) are used to compare image compression quality. The MSE represents the cumulative squared error between the compressed and the original image, whereas PSNR represents a measure of the peak error. \n",
    "\n",
    "PSNR\n",
    "\n",
    "PSNR is the most popular metric for measuring image reconstruction quality when lossy compression is utilized. In this instance, the signal is the original image data, and the compression error is noise. Consequently, PSNR approximates how humans perceive the quality of reconstruction when comparing compressed images. The PSNR computes the peak signal-to-noise ratio between two images in decibels. This ratio is used to compare the quality of the original and compressed image. The PSNR indicates the image quality of a compressed or reconstructed image.\n",
    "PSNR is defined as follows: \n",
    "\n",
    "\\begin{equation} PSNR = 10log_{10}(\\frac{(L - 1)^2}{MSE})= 20log_{10}(\\frac{L - 1}{RMSE}) \\end{equation}\n",
    "\n",
    "Here, L is the number of maximum possible intensity levels (minimum intensity level suppose to be 0) in an image.\n",
    "\n",
    "MSE\n",
    "\n",
    "The mean squared error, also known as MSE, is a statistical measure that calculates the average squared deviation between predicted values and actual values. As a result, we just compute the squared differences between each pixel individually. However, this will only work if we want to generate an image that has the highest degree of pixel color conformity to the actual image that we are working with. If the MSE value is lower, then the amount of error is also lower.\n",
    "\n",
    "MSE is defined as: \n",
    "\n",
    "\n",
    " \\begin{equation} MSE = \\frac{1}{mn}\\sum_{i=0}^{m-1}\\sum_{j=0}^{n-1}\\left ( O(i, j) - D(i, j)\\right )^{2}  \\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"results/res_e_bsds.jpg\" /> \n",
    "\\begin{align} fig 1 \\end{align}\n",
    "The esrgan model was trained using the BSDS200 dataset to produce the images that are currently being displayed above. The first picture is the original picture, and the second picture is one that was made by bicubic downsampling the first picture. The third picture is a model's prediction of what a super-resolution picture would look like.\n",
    "\n",
    "<img src=\"results/res_e_celeb.jpg\" /> \n",
    "\\begin{align} fig 2 \\end{align}\n",
    "The images displayed above were produced by the esrgan model trained on the celebA dataset. The first image is the original image, while the second image is generated by bicubic downsampling the original image. The third image is a model-predicted super-resolution image.\n",
    "\n",
    "The original image is clearer than the image that was predicted, but the image that was predicted is superior to the image that had a lower resolution. Despite the fact that the model's resolution is inferior to that of the original image. It achieved remarkable results while simultaneously improving the lr image's resolution.\n",
    "\n",
    "<img src=\"results/res_a_bsds.png\"/> \n",
    "\\begin{align} fig 3 \\end{align}\n",
    "The autoencoder model that was trained on the BSDS200 dataset was responsible for producing the images that are displayed above. The images in the first row are low-resolution versions of the original that have had noise superimposed on them. The images that are predicted by the autoencoder model are displayed as a row in the second signal.\n",
    "<img src=\"results/res_a_celeb.jpg\" />\n",
    "\\begin{align} fig 4 \\end{align}\n",
    "The images above were produced by an autoencoder trained on the celebA dataset. The images in the first row are low-resolution versions of the originals created by adding noise. The second signal row contains images predicted by the autoencoder model.\n",
    "\n",
    "In figure 3, the model performed poorly. The images appear identical to the human eye and have not improved. However, in fig.4 it is evident that there has been some improvement, albeit not as much as in the ESRGAN model.\n",
    "\n",
    "As can be seen here, ESRGAN has the capacity to generate high-quality super-resolution images using either the CelebA dataset or the BSDS200 dataset. The images that were generated are acceptable. Despite the lower image quality in comparison to the set used for training, our model performed very well (since we reshaped the images to become smaller and made them more blurry than the original ones). They are vivid enough to conjure up images of real people's faces for the celebA dataset and identify things on BSDS200 dataset.\n",
    "\n",
    "The autoencoders had a poor performance on the BSDS200 dataset, but their performance was acceptable on the celebA dataset. Both the reconstructed and the original versions of the images contain many elements in common with one another. On the other hand, the newly generated images have a certain amount of blurriness, and the ESRGAN seems to perform better in this regard. Perhaps the model requires additional training time in addition to some other hyperparameters in order to perform more effectively. There are many possible explanations, one of which is that the BSDS200 dataset is relatively small. As a consequence of this, the autoencoder did not receive adequate training data.\n",
    "\n",
    "\n",
    "By observing the figures above we can say that ESRGAN model worked better than the autoencoders. To evaluate the results mathematically, I used PSNR and MSE\n",
    "\n",
    "Peak signal-to-noise ratio (PSNR) is the ratio between the maximum possible power of an image and the power of corrupting noise that affects the quality of its representation.\n",
    "\n",
    "The PSNR of the fig 1 is 26.527180. \n",
    "\n",
    "The PSNR of the fig 2 is 29.614637.\n",
    "\n",
    "It is clear that fig 1 has a higher psnr than fig 2 does. This is because the celebA dataset contains more images that are very similar to each other. whereas the BSDS200 dataset contains fewer images but a wider variety of subject matter. So the model was able to train better in celebA dataset.\n",
    "\n",
    "The PSNR of the fig 3.1 is 82.342438. \n",
    "\n",
    "The PSNR of the fig 4.1 is 74.266418. \n",
    "\n",
    "When calculating PSNR, two images are compared to one another to establish the level of correspondence that exists between the two. When the psnr value is higher, it indicates that the images are more similar to one another. In this particular instance, the psnr values that were predicted by the autoencoder are higher than those that were observed in the ESRGAN images. This is because the autoencoder is unable to denoise the images and produce better outputs, which has resulted in this issue. In this particular scenario, the psnr is high due to the absence of a significant change, in contrast to the ESRGAN dataset, which does exhibit such a change.\n",
    "\n",
    "\n",
    "The mean squared error is a measure of how differently the original and compressed versions of an image compare to one another. In this section, I compared images with low resolution to images that were predicted. As a result, there is a significant difference between them.\n",
    "\n",
    "The MSE of the fig 1 is 436.43841145833335. \n",
    "\n",
    "The MSE of the fig 2 is 213.91999421296296.\n",
    "\n",
    "When the MSE is lower, it indicates that the images are closer together. We obtained high values, which indicates that the low resolution images are more dissimilar to the predicted images.\n",
    "\n",
    "The MSE of the fig 3.1 is 0.0011375164985656739. \n",
    "\n",
    "The MSE of the fig 4.1 is 0.007303977012634277. \n",
    "\n",
    "While determining MSE, two images are compared to one another to determine the degree of correspondence that exists between the two. This comparison helps establish the level of similarity between the images. It is an indication that the images are more similar to one another when the MSE value is lower. In this specific scenario, the MSE values that were predicted by the autoencoder have a lower absolute value than those that were discovered in the ESRGAN images. This is due to the fact that the autoencoder is unable to denoise the images and produce better outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ESRGAN algorithm outperformed the autoencoders. Autoencoders require more training time and require hyperparamenter tuning to function optimally. \n",
    "\n",
    "The most difficult part in this project is handling image data and evaluating the results. I had hard time understanding why my mse is high and psnr is low for better looking images. It took me a considerable amount of time to realize that low-resolution images are transforming into high-resolution images, indicating that they are dissimilar and have high mse and low psnr values.\n",
    "Handling image data is a big task. Working with 3D images converting the images into arrays and adding noise to them took a respectable amount of my time. The most difficult aspect is visualizing it. (But the best part is also visualizing it!!)\n",
    "\n",
    "Autoencoders are able to produce higher-quality images using the celebA dataset as opposed to the BSDS200 dataset. Because of this, the performance of auto encoders is improved when there is both a large quantity of data and more time spent training them.\n",
    "\n",
    "ESRGAN achieved better outcomes with both datasets and is a more reliable option. Even when the image is distorted more than usual, they are still able to produce satisfactory results. As a result, GANs are superior to autoencoders in this regard.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network](https://arxiv.org/pdf/1609.04802v5.pdf),Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi, CVPR 2017.\n",
    "*[Image Super-Resolution With Deep Variational Autoencoders](https://arxiv.org/abs/2203.09445),Darius Chira, Ilian Haralampiev, Ole Winther, Andrea Dittadi, Valentin Liévin, 26 Oct 2022\n",
    "* [Comprehensive Introduction to Autoencoders](https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368)\n",
    "* [Introduction to Turing Learning and GANs](https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368)\n",
    "*[Image Super Resolution using ESRGAN](https://www.tensorflow.org/hub/tutorials/image_enhancing)\n",
    "*[Image Super-Resolution using Convolution Neural Networks and Auto-encoders](https://towardsdatascience.com/image-super-resolution-using-convolution-neural-networks-and-auto-encoders-28c9eceadf90) \n",
    "*[Adding Noise to Image Data for Deep Learning Data Augmentation](https://debuggercafe.com/adding-noise-to-image-data-for-deep-learning-data-augmentation/)\n",
    "*[Why to Add Noise to Images for Machine Learning](https://blog.roboflow.com/why-to-add-noise-to-images-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-11T20:24:09.713104Z",
     "start_time": "2022-10-11T20:24:09.678375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file project_report.ipynb is 2530\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import nbformat\n",
    "import glob\n",
    "nbfile = glob.glob('project_report.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = nbformat.read(f, nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
